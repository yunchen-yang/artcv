{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Dataset Visualization\n### 1) Goal\nBefore constructing the classification model, let's visualize the dataset and define the task in the first place. <br>\n<b>Step 1:</b> Load and check the labels and training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nroot_path = '/kaggle/input/imet-2020-fgvc7/'\nlabels = pd.read_csv(root_path+'labels.csv')\ndata = pd.read_csv(root_path+'train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"labels.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Step 2:</b> Split the attribute names into catagories and types."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _split_attr_names(attr_name_str):\n    return [item.strip() for item in attr_name_str.split('::')] \n\ndef _split_labels(labels_dataframe):\n    split_labels_dict = dict(attribute_id=[], attr_tier1=[], attr_tier2=[])\n    for i in range(labels_dataframe.shape[0]):\n        tem = _split_attr_names(labels['attribute_name'][i])\n        split_labels_dict['attribute_id'].append(labels['attribute_id'][i])\n        split_labels_dict['attr_tier1'].append(tem[0])\n        split_labels_dict['attr_tier2'].append(tem[1]) \n    split_labels = pd.DataFrame(split_labels_dict, \n                                columns=['attribute_id', 'attr_tier1', 'attr_tier2'])\n    return split_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_labels = _split_labels(labels)\nsplit_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catagories = sorted(list(set(list(split_labels['attr_tier1']))))\nprint(len(catagories))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are in total 5 catagories: %s, %s, %s, %s, %s.'\n     %(catagories[0],\n       catagories[1],\n       catagories[2],\n       catagories[3],\n       catagories[4]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Step 3:</b> Define the types of classification tasks."},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_indexer_coarse(labels_dataframe):\n    split_labels_dict = dict(attribute_id=[], attr_tier1=[], attr_tier2=[])\n    for i in range(labels_dataframe.shape[0]):\n        tem = [item.strip() for item in labels_dataframe['attribute_name'][i].split('::')] \n        split_labels_dict['attribute_id'].append(labels_dataframe['attribute_id'][i])\n        split_labels_dict['attr_tier1'].append(tem[0])\n        split_labels_dict['attr_tier2'].append(tem[1])\n        \n    split_labels = pd.DataFrame(split_labels_dict, \n                                columns=['attribute_id', 'attr_tier1', 'attr_tier2'])\n    \n    tier1 = dict()\n    tier2 = dict()\n    attr2indexing = dict()\n    indexing2attr = dict()\n    label_indexing_list = []\n    for i_1, item1 in enumerate(sorted(list(set(list(split_labels['attr_tier1']))))):\n        assert len(list(set(list(split_labels['attr_tier2'][split_labels['attr_tier1']==item1])))) \\\n        == len(list(split_labels['attr_tier2'][split_labels['attr_tier1']==item1]))\n        tier1[item1] = i_1\n        tier2[item1] = dict()\n        list_tem = list(split_labels['attr_tier2'][split_labels['attr_tier1']==item1])\n        for i_2, item2 in enumerate(list_tem):\n            tier2[item1][item2] = i_2 + 1\n    for idx in range(split_labels.shape[0]):\n        tier1_idx = tier1[split_labels['attr_tier1'][idx]]\n        tier2_idx = tier2[split_labels['attr_tier1'][idx]][split_labels['attr_tier2'][idx]]\n        label_indexing_list.append([tier1_idx, tier2_idx])\n        attr2indexing[split_labels['attribute_id'][idx]] = [tier1_idx, tier2_idx]\n        indexing2attr[str([tier1_idx, tier2_idx])] = split_labels['attribute_id'][idx]\n    labels_indexing_df = split_labels.copy()\n    labels_indexing_df['indexing'] = label_indexing_list\n    return labels_indexing_df, attr2indexing, indexing2attr\nlabels_indexing_df, attr2indexing, indexing2attr = label_indexer_coarse(labels)\n\ncount_array = np.zeros((data.shape[0], 6), dtype='int')\nfor i, attr_ids in enumerate(data['attribute_ids']):\n    cata_indexing = [labels_indexing_df['indexing'][int(attr_id)][0] \n                     for attr_id in data['attribute_ids'][i].split()]\n    for i_ in range(count_array.shape[1]):\n        count_array[i, i_] = cata_indexing.count(i_)\nfor i_tier1, tier1 in enumerate(sorted(list(set(list(split_labels['attr_tier1']))))):\n    print('Catagory: %s; minimum count of tagging per item: %s; maximum count of tagging per item: %s' \n          %(tier1, np.amin(count_array, axis=0)[i_tier1], np.amax(count_array, axis=0)[i_tier1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Tasks\nAs the counting results shown, an object can only have one single label in catagory \"dimension\", but it may have multiple labels in another 4 catagories.<br>\nHence, we define the classification task of catagory \"dimension\" as multi-class classification and the tasks of the other 4 catagories as multi-label classification. <br>\nWe decided to train a multi-head CNN model to complete this task. An pretrained ResNet architecture will co-train with 5 classifier heads. The second classifier will perform the multi-class classification (In total 6 classes; 0 means no label being eligible). The other 4 classifiers will perform multi-label classification.  "},{"metadata":{},"cell_type":"markdown","source":"## 2. Model constructing and training\nlet's start constructing the classification model.\n### 1) Library and codes\nThe codes are also available on GitHub (https://github.com/yunchen-yang/artcv)."},{"metadata":{},"cell_type":"markdown","source":"#### Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nfrom torchvision.transforms import Compose, Resize, RandomResizedCrop, Normalize, ToTensor\nimport pandas as pd\nimport torch\n\n\ndef label_indexer_fine(labels_dataframe):\n    split_labels_dict = dict(attribute_id=[], attr_tier1=[], attr_tier2=[], attr_tier3=[])\n    for i in range(labels_dataframe.shape[0]):\n        tem = [item.strip() for split_list in [item_.split(';')\n                                               for item_ in labels_dataframe['attribute_name'][i].split('::')]\n               for item in split_list]\n        split_labels_dict['attribute_id'].append(labels_dataframe['attribute_id'][i])\n        split_labels_dict['attr_tier1'].append(tem[0])\n        split_labels_dict['attr_tier2'].append(tem[1])\n        try:\n            split_labels_dict['attr_tier3'].append(tem[2])\n        except:\n            split_labels_dict['attr_tier3'].append('None')\n    split_labels = pd.DataFrame(split_labels_dict,\n                                columns=['attribute_id', 'attr_tier1', 'attr_tier2', 'attr_tier3'])\n\n    tier1 = dict()\n    tier2 = dict()\n    counting_dict = dict()\n    attr2indexing = dict()\n    indexing2attr = dict()\n    label_indexing_list = []\n    for i_1, item1 in enumerate(sorted(list(set(list(split_labels['attr_tier1']))))):\n        tier1[item1] = i_1\n        tier2[item1] = dict()\n        list_tem = sorted(list(set(list(split_labels['attr_tier2'][split_labels['attr_tier1'] == item1]))))\n        for i_2, item2 in enumerate(list_tem):\n            tier2[item1][item2] = i_2 + 1\n        counting_dict[item1] = np.ones(len(list_tem), dtype='int')\n    for idx in range(split_labels.shape[0]):\n        tier1_idx = tier1[split_labels['attr_tier1'][idx]]\n        tier2_idx = tier2[split_labels['attr_tier1'][idx]][split_labels['attr_tier2'][idx]]\n        tier3_idx = counting_dict[split_labels['attr_tier1'][idx]][tier2_idx - 1]\n        counting_dict[split_labels['attr_tier1'][idx]][tier2_idx - 1] += 1\n        label_indexing_list.append([tier1_idx, tier2_idx, tier3_idx])\n        attr2indexing[split_labels['attribute_id'][idx]] = [tier1_idx, tier2_idx, tier3_idx]\n        indexing2attr[str([tier1_idx, tier2_idx, tier3_idx])] = split_labels['attribute_id'][idx]\n    labels_indexing_df = split_labels.copy()\n    labels_indexing_df['indexing'] = label_indexing_list\n    return labels_indexing_df, attr2indexing, indexing2attr\n\n\ndef label_indexer_coarse(labels_dataframe):\n    split_labels_dict = dict(attribute_id=[], attr_tier1=[], attr_tier2=[])\n    for i in range(labels_dataframe.shape[0]):\n        tem = [item.strip() for item in labels_dataframe['attribute_name'][i].split('::')]\n        split_labels_dict['attribute_id'].append(labels_dataframe['attribute_id'][i])\n        split_labels_dict['attr_tier1'].append(tem[0])\n        split_labels_dict['attr_tier2'].append(tem[1])\n\n    split_labels = pd.DataFrame(split_labels_dict,\n                                columns=['attribute_id', 'attr_tier1', 'attr_tier2'])\n\n    tier1 = dict()\n    tier2 = dict()\n    attr2indexing = dict()\n    indexing2attr = dict()\n    label_indexing_list = []\n    for i_1, item1 in enumerate(sorted(list(set(list(split_labels['attr_tier1']))))):\n        assert len(list(set(list(split_labels['attr_tier2'][split_labels['attr_tier1'] == item1])))) \\\n               == len(list(split_labels['attr_tier2'][split_labels['attr_tier1'] == item1]))\n        tier1[item1] = i_1\n        tier2[item1] = dict()\n        list_tem = list(split_labels['attr_tier2'][split_labels['attr_tier1'] == item1])\n        for i_2, item2 in enumerate(list_tem):\n            tier2[item1][item2] = i_2\n    for idx in range(split_labels.shape[0]):\n        tier1_idx = tier1[split_labels['attr_tier1'][idx]]\n        tier2_idx = tier2[split_labels['attr_tier1'][idx]][split_labels['attr_tier2'][idx]]\n        label_indexing_list.append([tier1_idx, tier2_idx])\n        attr2indexing[split_labels['attribute_id'][idx]] = [tier1_idx, tier2_idx]\n        indexing2attr[str([tier1_idx, tier2_idx])] = split_labels['attribute_id'][idx]\n    labels_indexing_df = split_labels.copy()\n    labels_indexing_df['indexing'] = label_indexing_list\n    return labels_indexing_df, attr2indexing, indexing2attr\n\n\ndef imgreader(img_id, ext, path, attr_ids, attr2indexing, length_list, dimension=256,\n              task=('ml', 'ml', 'mc', 'ml', 'ml'), transform='val', grey_scale=False):\n    file_path = f'{path}/{img_id}.{ext}'\n    with open(file_path, 'rb') as f:\n        img_ = Image.open(f)\n        if grey_scale:\n            img = img_.convert('L')\n        else:\n            img = img_.convert('RGB')\n\n    transformer = {\n        'train': Compose([RandomResizedCrop(size=(dimension, dimension)),\n                          ToTensor(),\n                          Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n                         ),\n        'val': Compose([Resize(size=(dimension, dimension)),\n                        ToTensor(),\n                        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n    }\n    x = transformer[transform](img)\n    y_list = [attr2indexing[int(attr_id)] for attr_id in attr_ids.split()]\n    y_dict = labels_list2array(y_list, length_list, task)\n    return x, tuple(y_dict.values())\n\n\ndef counting_elements(labels_indexing_df):\n    return [len(labels_indexing_df['indexing'][labels_indexing_df['attr_tier1']==catagory])\n            for catagory in sorted(list(set(list(labels_indexing_df['attr_tier1']))))]\n\n\ndef labels_list2array(y_list, length_list, task):\n    y_dict = dict()\n    for i in range(len(task)):\n        if task[i] != 'mc':\n            y_dict[i] = torch.FloatTensor(np.zeros(length_list[i]))\n        else:\n            y_dict[i] = torch.LongTensor([0])\n\n    for idx_list in y_list:\n        if task[idx_list[0]] != 'mc':\n            y_dict[idx_list[0]][idx_list[1]] = 1\n        else:\n            y_dict[idx_list[0]][0] = idx_list[1]+1\n\n    return y_dict\n\n\ndef image_list_scan(data_info, indices):\n    if indices is None:\n        return list(data_info['id']), list(data_info['attribute_ids'])\n    else:\n        return list(data_info['id'][indices]), list(data_info['attribute_ids'][indices])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### datatool"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nimport sys\nimport math\n\n\nclass ImgDataset(Dataset):\n    def __init__(self, x, y, path, attr2indexing, length_list, task,\n                 ext='png', dimension=256, transform='val', grey_scale=False):\n        super().__init__()\n        self.x = x\n        self.y = y\n        self.path = path\n        self.attr2indexing =attr2indexing\n        self.length_list = length_list\n        self.task = task\n        self.ext = ext\n        self.dimension = dimension\n        self.transform = transform\n        self.grey_scale = grey_scale\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, index):\n        img, ys = imgreader(self.x[index], self.ext, self.path, self.y[index],\n                            self.attr2indexing, self.length_list,\n                            self.dimension, self.task, self.transform, self.grey_scale)\n        y0, y1, y2, y3, y4 = ys\n\n        return img, y0, y1, y2, y3, y4\n\n\nclass TrainValSet:\n    def __init__(self, ext='png', path=None, indices=None, dimension=256, data_info_path=None, labels_info_path=None,\n                 task=('ml', 'ml', 'mc', 'ml', 'ml'), train_val_split=0.7, seed=0):\n        super().__init__()\n        self.ext = ext\n        self.dimension = dimension\n        self.indices = indices\n        self.seed = seed\n\n        if path is None:\n            self.path = f'{sys.path[0]}/train'\n        else:\n            self.path = path\n\n        if data_info_path is None:\n            self.data_info_path = f'{sys.path[0]}/train.csv'\n        else:\n            self.data_info_path = data_info_path\n\n        if labels_info_path is None:\n            self.labels_info_path = f'{sys.path[0]}/labels.csv'\n        else:\n            self.labels_info_path = labels_info_path\n\n        self.labels_info = pd.read_csv(self.labels_info_path)\n        self.data_info = pd.read_csv(self.data_info_path)\n\n        self.labels_indexing_df, self.attr2indexing, self.indexing2attr = label_indexer_coarse(self.labels_info)\n        self.length_list = counting_elements(self.labels_indexing_df)\n        self.X_all, self.Y_all = image_list_scan(self.data_info, indices=self.indices)\n        self.task = task\n\n        self.all = ImgDataset(self.X_all, self.Y_all, self.path, self.attr2indexing, self.length_list,\n                              task=self.task, ext=self.ext, dimension=256, transform='val', grey_scale=False)\n        self.train_val_split = train_val_split\n        if bool(self.train_val_split):\n            assert(0 < self.train_val_split < 1)\n            num_train = math.ceil(len(self.X_all) * self.train_val_split)\n            np.random.seed(seed=self.seed)\n            indices_array = np.random.permutation(len(self.X_all))\n            self.X_train = [self.X_all[i] for i in indices_array[:num_train]]\n            self.Y_train = [self.Y_all[i] for i in indices_array[:num_train]]\n            self.train = ImgDataset(self.X_train, self.Y_train, self.path, self.attr2indexing, self.length_list,\n                                    task=self.task, ext=self.ext, dimension=256, transform='train', grey_scale=False)\n            self.X_val = [self.X_all[i] for i in indices_array[num_train:]]\n            self.Y_val = [self.Y_all[i] for i in indices_array[num_train:]]\n            self.val = ImgDataset(self.X_val, self.Y_val, self.path, self.attr2indexing, self.length_list,\n                                   task=self.task, ext=self.ext, dimension=256, transform='val', grey_scale=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.models.resnet import ResNet\nimport torch\nfrom torch import nn as nn\nimport collections\nimport torch.nn.functional as F\n\n\nclass ResNet_CNN(ResNet):\n    def __init__(self, block, layers, weight_path, **kwargs):\n        super().__init__(block, layers, **kwargs)\n        self.weight_path = weight_path\n        if self.weight_path is not None:\n            self.load_state_dict(torch.load(self.weight_path))\n        del self.fc\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        return x\n\n\nclass Classifier(nn.Module):\n    def __init__(self, dim_in, dim_out, dim_hidden, n_layers, task='ml', use_batch_norm=True, dropout_rate=0.01):\n        super().__init__()\n        dims = [dim_in] + [dim_hidden]*(n_layers-1) + [dim_out]\n        self.task = task\n        self.use_batch_norm = use_batch_norm\n        self.dropout_rate = dropout_rate\n        self.classifier = nn.Sequential(collections.OrderedDict(\n            [('Layer {}'.format(i), nn.Sequential(\n                nn.Linear(n_in, n_out),\n                nn.BatchNorm1d(n_out, momentum=.01, eps=0.001) if self.use_batch_norm else None,\n                nn.ReLU() if i != len(dims)-2 else None,\n                nn.Dropout(p=self.dropout_rate) if self.dropout_rate > 0 else None))\n             for i, (n_in, n_out) in enumerate(zip(dims[:-1], dims[1:]))]))\n\n    def get_logits(self, x):\n        for layers in self.classifier:\n            for layer in layers:\n                if layer is not None:\n                    x = layer(x)\n        return x\n\n    def forward(self, x):\n        if self.task == 'mc':\n            return F.softmax(self.get_logits(x), dim=-1)\n        elif self.task == 'ml':\n            return torch.sigmoid(self.get_logits(x))\n        else:\n            raise ValueError(\"The task tag must be either 'ml' (multi-label) or 'mc' (multi-class)!\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models.resnet as resnet\nimport collections\nimport torch.nn.functional as F\n\n\nBLOCK = {'18': 'BasicBlock',\n         '34': 'BasicBlock',\n         '50': 'Bottleneck',\n         '101': 'Bottleneck',\n         '152': 'Bottleneck'}\n\n\nLAYERS = {'18': [2, 2, 2, 2],\n          '34': [3, 4, 6, 3],\n          '50': [3, 4, 6, 3],\n          '101': [3, 4, 23, 3],\n          '152': [3, 8, 36, 3]}\n\n\nclass ArtCV(nn.Module):\n    def __init__(self, tag='18', num_labels=(100, 681, 6, 1920, 768),\n                 classifier_layers=(1, 1, 1, 1, 1), classifier_hidden=(2048, 2048, 2048, 2048, 2048),\n                 task=('ml', 'ml', 'mc', 'ml', 'ml'), weights=(1, 1, 1, 1, 1),\n                 use_batch_norm=True, dropout_rate=0.01,\n                 weight_path=None, freeze_cnn=False):\n        super().__init__()\n        self.tag = tag\n        self.num_labels = num_labels\n        self.classifier_layers = classifier_layers\n        self.classifier_hidden = classifier_hidden\n        self.task = task\n        self.weights = weights\n        self.use_batch_norm = use_batch_norm\n        self.dropout_rate = dropout_rate\n        self.weight_path = weight_path\n        self.freeze_cnn = freeze_cnn\n\n        self.cnn = ResNet_CNN(getattr(resnet, BLOCK[tag]), LAYERS[tag], self.weight_path)\n        if self.freeze_cnn:\n            for p in self.cnn.parameters():\n                p.requires_grad = False\n        self.classifiers = nn.ModuleDict(\n            collections.OrderedDict(\n                [('classifier{}'.format(i), Classifier(dim_in=512 * getattr(resnet, BLOCK[tag]).expansion,\n                                                       dim_out=self.num_labels[i],\n                                                       dim_hidden=self.classifier_hidden[i],\n                                                       n_layers=self.classifier_layers[i],\n                                                       task=self.task[i],\n                                                       use_batch_norm=self.use_batch_norm,\n                                                       dropout_rate=self.dropout_rate))\n                 for i in range(len(self.num_labels))]))\n\n    def inference(self, x):\n        x_features = self.cnn(x)\n        if self.freeze_cnn:\n            return x_features.detach()\n        else:\n            return x_features\n\n    def get_probs(self, x):\n        x_features = self.inference(x)\n        y_pred0 = self.classifiers['classifier0'](x_features)\n        y_pred1 = self.classifiers['classifier1'](x_features)\n        y_pred2 = self.classifiers['classifier2'](x_features)\n        y_pred3 = self.classifiers['classifier3'](x_features)\n        y_pred4 = self.classifiers['classifier4'](x_features)\n        return y_pred0.view(-1, self.num_labels[0]), y_pred1.view(-1, self.num_labels[1]), \\\n               y_pred2.view(-1, self.num_labels[2]), \\\n               y_pred3.view(-1, self.num_labels[3]), y_pred4.view(-1, self.num_labels[4])\n\n    def get_loss(self, x, y0, y1, y2, y3, y4):\n        y_pred0, y_pred1, y_pred2, y_pred3, y_pred4 = self.get_probs(x)\n        loss0 = torch.mean(F.binary_cross_entropy(y_pred0, y0, reduction='none'), dim=1)\n        loss1 = torch.mean(F.binary_cross_entropy(y_pred1, y1, reduction='none'), dim=1)\n        loss3 = torch.mean(F.binary_cross_entropy(y_pred3, y3, reduction='none'), dim=1)\n        loss4 = torch.mean(F.binary_cross_entropy(y_pred4, y4, reduction='none'), dim=1)\n        loss2 = F.cross_entropy(y_pred2, y2.view(-1), reduction='none')\n        return loss0, loss1, loss2, loss3, loss4\n\n    def forward(self, x, y0, y1, y2, y3, y4):\n        loss0, loss1, loss2, loss3, loss4 = self.get_loss(x, y0, y1, y2, y3, y4)\n        loss = loss0*self.weights[0] + loss1*self.weights[1] + loss2*self.weights[2] + \\\n               loss3*self.weights[3] + loss4*self.weights[4]\n        return loss\n\n    def get_concat_probs(self, x):\n        y_pred0, y_pred1, y_pred2, y_pred3, y_pred4 = self.get_probs(x)\n\n        return torch.cat((y_pred0, y_pred1,\n                          F.one_hot(y_pred2.argmax(axis=-1), num_classes=self.num_labels[2]).float()[:, 1:],\n                          y_pred3, y_pred4), dim=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### trainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport sys\nimport copy\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import fbeta_score\n\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nfrom tqdm import trange\n\n\nclass Trainer:\n    def __init__(self, model, dataset, use_cuda=True,\n                 shuffle=True, epochs=100, batch_size_train=64,\n                 monitor_frequency=5, compute_acc=True, printout=False,\n                 thre=(0.04, 0.06, 0.08, 0.06), batch_size_val=64,\n                 dataloader_train_kwargs=dict(), dataloader_val_kwargs=dict(),\n                 batch_size_all=64, dataloader_all_kwargs=dict()):\n        self.model = model\n        self.dataset = dataset\n        self.train_val = bool(self.dataset.train_val_split)\n        self.use_cuda = use_cuda\n        self.epochs = epochs\n        self.running_loss = []\n\n        if self.train_val:\n            if shuffle:\n                train_sampler = RandomSampler(dataset.train)\n                val_sampler = RandomSampler(dataset.val)\n            else:\n                train_sampler = SequentialSampler(dataset.train)\n                val_sampler = SequentialSampler(dataset.val)\n            self.dataloader_train_kwargs = copy.deepcopy(dataloader_train_kwargs)\n            self.dataloader_train_kwargs.update({'batch_size': batch_size_train, 'sampler': train_sampler})\n            self.dataloader_train = DataLoader(self.dataset.train, **self.dataloader_train_kwargs)\n            self.dataloader_val_kwargs = copy.deepcopy(dataloader_val_kwargs)\n            self.dataloader_val_kwargs.update({'batch_size': batch_size_val, 'sampler': val_sampler})\n            self.dataloader_val = DataLoader(self.dataset.val, **self.dataloader_val_kwargs)\n            self.loss_history_train = []\n            self.loss_history_val = []\n            self.accuracy_history_train = []\n            self.accuracy_history_val = []\n\n        else:\n            sampler = RandomSampler(dataset.all)\n            self.dataloader_train_kwargs = copy.deepcopy(dataloader_train_kwargs)\n            self.dataloader_train_kwargs.update({'batch_size': batch_size_train, 'sampler': sampler})\n            self.dataloader_train = DataLoader(self.dataset.all, **self.dataloader_train_kwargs)\n            self.loss_history_train = []\n            self.accuracy_history_train = []\n\n        all_sampler = SequentialSampler(dataset.all)\n        self.dataloader_all_kwargs = copy.deepcopy(dataloader_all_kwargs)\n        self.dataloader_all_kwargs.update({'batch_size': batch_size_all, 'sampler': all_sampler})\n        self.dataloader_all = DataLoader(self.dataset.all, **self.dataloader_all_kwargs)\n\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        if self.use_cuda:\n            self.model.cuda()\n        self.monitor_frequency = monitor_frequency\n        self.compute_acc = compute_acc\n        self.printout = printout\n        self.thre = thre\n\n    def before_iter(self):\n        pass\n\n    def after_iter(self):\n        pass\n\n    def train(self, parameters=None, lr=1e-1, betas=(0.9, 0.999), eps=1e-8, weight_decay=0,\n              reduce_lr=False, step=5, gamma=0.8,\n              grad_clip=False, max_norm=1e-5):\n        epochs = self.epochs\n        self.model.train()\n        params = filter(lambda x: x.requires_grad, self.model.parameters()) \\\n            if parameters is None else parameters\n        optim = torch.optim.Adam(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n\n        with trange(epochs, desc='Training progress: ', file=sys.stdout) as progbar:\n            for epoch_idx in progbar:\n                self.before_iter()\n                progbar.update(1)\n                running_loss = 0\n                for data_tensors in self.dataloader_train:\n                    data_tensor_tuples = [data_tensors]\n                    loss = self.loss(*data_tensor_tuples)\n                    running_loss += loss.item()\n                    optim.zero_grad()\n                    loss.backward()\n                    if grad_clip:\n                        torch.nn.utils.clip_grad_norm_(params, max_norm=max_norm)\n                    optim.step()\n                self.running_loss.append(running_loss/len(self.dataloader_train))\n\n                if (epoch_idx+1) % step == 0 & reduce_lr:\n                    for p in optim.param_groups:\n                        p['lr'] *= gamma\n\n                if (epoch_idx + 1) % self.monitor_frequency == 0:\n                    current_loss_train = self.compute_loss(tag='train')\n                    self.loss_history_train.append(current_loss_train)\n                    if self.compute_acc:\n                        current_accuracy_train = self.compute_accuracy(tag='train')\n                        self.accuracy_history_train.append(current_accuracy_train)\n                    if self.train_val:\n                        current_loss_val = self.compute_loss(tag='val')\n                        self.loss_history_val.append(current_loss_val)\n                        if self.compute_acc:\n                            current_accuracy_val = self.compute_accuracy(tag='val')\n                            self.accuracy_history_val.append(current_accuracy_val)\n                    if self.printout:\n                        print(\"After %i epochs, loss is %f and prediction accuracy is %f.\"\n                              % (epoch_idx, current_loss_train, current_accuracy_train))\n                self.after_iter()\n\n    def loss(self, data_tensors):\n        x, y0, y1, y2, y3, y4 = data_tensors\n        if self.use_cuda and torch.cuda.is_available():\n            x = x.cuda()\n            y0 = y0.cuda()\n            y1 = y1.cuda()\n            y2 = y2.cuda()\n            y3 = y3.cuda()\n            y4 = y4.cuda()\n        loss = torch.mean(self.model(x, y0, y1, y2, y3, y4))\n        return loss\n\n    @torch.no_grad()\n    def plot_running_loss(self, epochs_override=None):\n        len_ticks = len(self.running_loss)\n        if epochs_override is None:\n            x_axis = np.linspace(0, self.epochs, len_ticks)\n        else:\n            x_axis = np.linspace(0, epochs_override, len_ticks)\n        plt.figure()\n        plt.plot(x_axis, self.running_loss)\n        plt.xlabel('Number of epochs')\n        plt.ylabel('Estimated loss')\n        plt.show()\n\n    @torch.no_grad()\n    def compute_loss(self, tag):\n        self.model.eval()\n        loss_sum = 0\n        if tag == 'train':\n            _dataloader = self.dataloader_train\n        elif tag == 'val':\n            _dataloader = self.dataloader_val\n        elif tag == 'all':\n            _dataloader = self.dataloader_all\n        else:\n            raise ValueError('Invalid tag!')\n        _dataset = _dataloader.dataset\n        for data_tensors in _dataloader:\n            x, y0, y1, y2, y3, y4 = data_tensors\n            if self.use_cuda and torch.cuda.is_available():\n                x = x.cuda()\n                y0 = y0.cuda()\n                y1 = y1.cuda()\n                y2 = y2.cuda()\n                y3 = y3.cuda()\n                y4 = y4.cuda()\n            loss = self.model(x, y0, y1, y2, y3, y4)\n            loss_sum += torch.sum(loss).item()\n        loss_mean = loss_sum / len(_dataset)\n        self.model.train()\n        return loss_mean\n\n    @torch.no_grad()\n    def loss_history_plot(self, epochs_override=None):\n        len_ticks = len(self.loss_history_train)\n        if epochs_override is None:\n            x_axis = np.linspace(0, self.epochs, len_ticks)\n        else:\n            x_axis = np.linspace(0, epochs_override, len_ticks)\n        plt.figure()\n        if self.train_val:\n            assert (len(self.loss_history_train) == len(self.loss_history_val))\n            plt.plot(x_axis, self.loss_history_train, label='Training set')\n            plt.plot(x_axis, self.loss_history_val, label='Validation set')\n            plt.legend()\n        else:\n            plt.plot(x_axis, self.loss_history_train)\n\n        plt.xlabel('Number of epochs')\n        plt.ylabel('Loss')\n        plt.show()\n\n    @torch.no_grad()\n    def get_probs(self, tag, test=False):\n        self.model.eval()\n        predictions_tem = []\n        ground_truth = []\n        if tag == 'train':\n            _dataloader = self.dataloader_train\n        elif tag == 'val':\n            _dataloader = self.dataloader_val\n        elif tag == 'all':\n            _dataloader = self.dataloader_all\n        else:\n            raise ValueError('Invalid tag!')\n        for data_tensors in _dataloader:\n            if not test:\n                x, y0, y1, y2, y3, y4 = data_tensors\n                if self.use_cuda and torch.cuda.is_available():\n                    x = x.cuda()\n                    y0 = y0.cuda()\n                    y1 = y1.cuda()\n                    y2 = y2.cuda()\n                    y3 = y3.cuda()\n                    y4 = y4.cuda()\n                ground_truth += [torch.cat((y0.long(),\n                                            y1.long(),\n                                            F.one_hot(y2, num_classes=6).squeeze()[:, 1:].long(),\n                                            y3.long(),\n                                            y4.long()), dim=1)]\n            else:\n                x = data_tensors\n                if self.use_cuda and torch.cuda.is_available():\n                    x = x.cuda()\n            y_concat_prob = self.model.get_concat_probs(x)\n            predictions_tem += [y_concat_prob]\n        predictions_array = torch.cat(predictions_tem).detach().cpu().numpy()\n        self.model.train()\n        if not test:\n            return torch.cat(ground_truth).detach().cpu().numpy(), predictions_array\n        else:\n            return predictions_array\n\n    @torch.no_grad()\n    def make_predictions(self, tag, thre,\n                         boundary=([0, 100], [100, 781], [786, 2706], [2706, 3474])):\n        assert len(thre) == len(boundary)\n        ground_truth, predictions_array = self.get_probs(tag=tag)\n        predictions = np.zeros(predictions_array.shape, dtype='int')\n\n        for i in range(len(boundary)):\n            predictions[:, boundary[i][0]: boundary[i][1]][\n                predictions_array[:, boundary[i][0]: boundary[i][1]] > thre[i]] = 1\n        return ground_truth, predictions\n\n    @torch.no_grad()\n    def compute_accuracy(self, tag):\n        y_true, y_pred = self.make_predictions(tag=tag, thre=self.thre)\n        f_beta = [fbeta_score(y_true[i, :], y_pred[i, :], beta=2) for i in range(y_true.shape[0])]\n        return sum(f_beta) / len(f_beta)\n\n    @torch.no_grad()\n    def accuracy_history_plot(self, epochs_override=None):\n        len_ticks = len(self.accuracy_history_train)\n        if epochs_override is None:\n            x_axis = np.linspace(0, self.epochs, len_ticks)\n        else:\n            x_axis = np.linspace(0, epochs_override, len_ticks)\n        plt.figure()\n        if self.train_val:\n            assert (len(self.accuracy_history_train) == len(self.accuracy_history_val))\n            plt.plot(x_axis, self.accuracy_history_train, label='Training set')\n            plt.plot(x_axis, self.accuracy_history_val, label='Validation set')\n            plt.legend()\n        else:\n            plt.plot(x_axis, self.accuracy_history_train)\n\n        plt.xlabel('Number of epochs')\n        plt.ylabel('Accuracy')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_root_path = '/kaggle/input/imet-2020-fgvc7'\npath = f'{_root_path}/train'\ndata_info_path = f'{_root_path}/train.csv'\nlabels_info_path = f'{_root_path}/labels.csv'\ndataset = TrainValSet(train_val_split=0.9, path=path, data_info_path=data_info_path, labels_info_path=labels_info_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag='50'\nweight_path = f'/kaggle/input/resnet{tag}/resnet{tag}.pth'\nmodel = ArtCV(tag=tag, weight_path=weight_path, freeze_cnn=True, dropout_rate=0, weights=(1, 1, 2, 1, 1), classifier_layers=(2, 2, 1, 2, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = Trainer(model, dataset, batch_size_train=512, batch_size_val=512, batch_size_all=512,\n                  epochs=15, compute_acc=False,\n                  monitor_frequency=100,\n                  dataloader_train_kwargs={'num_workers':2}, dataloader_val_kwargs={'num_workers':2},\n                  dataloader_all_kwargs={'num_workers':2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.train(reduce_lr=True)\nfile_name = 'frozen_resnet50_2layer_15epochs_reduced_lr.model.pkl'\nsave_path = f'{sys.path[0]}/{file_name}'\ntorch.save(trainer.model.state_dict(), save_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.plot_running_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_root_path = '/kaggle/input/imet-2020-fgvc7'\npath = f'{_root_path}/train'\ndata_info_path = f'{_root_path}/train.csv'\nlabels_info_path = f'{_root_path}/labels.csv'\ndataset = TrainValSet(train_val_split=False, path=path, data_info_path=data_info_path, labels_info_path=labels_info_path)\n\n# tag='50'\n# weight_path = f'/kaggle/input/resnet{tag}/resnet{tag}.pth'\n# model = ArtCV(tag=tag, weight_path=weight_path, freeze_cnn=True, dropout_rate=0, weights=(1, 1, 2, 1, 1), classifier_layers=(2, 2, 1, 2, 2))\n\ntrainer = Trainer(model, dataset, batch_size_train=512, batch_size_val=512, batch_size_all=512,\n                  epochs=12, compute_acc=False,\n                  monitor_frequency=3,\n                  dataloader_train_kwargs={'num_workers':2}, dataloader_val_kwargs={'num_workers':2},\n                  dataloader_all_kwargs={'num_workers':2})\n\n# file_name = 'frozen_resnet50_2layer_15epochs.model.pkl'\n# save_path = f'/kaggle/input/pretrained/{file_name}'\n# model.load_state_dict(torch.load(save_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gt, probs = trainer.get_probs(tag='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import fbeta_score\n\ndef f2score(ground_truth, probs, thre, boundary=([0, 100], [100, 781], [786, 2706], [2706, 3474]), return_mean=True):\n    if len(thre) == 1:\n        thre = (thre, thre, thre, thre)\n    predictions = np.zeros(probs.shape, dtype='int')\n    for i in range(len(boundary)):\n        predictions[:, boundary[i][0]: boundary[i][1]][\n            probs[:, boundary[i][0]: boundary[i][1]] > thre[i]] = 1\n    f_beta = [fbeta_score(ground_truth[i,:], predictions[i,:], beta=2) for i in range(ground_truth.shape[0])]\n    if return_mean:\n        return sum(f_beta)/len(f_beta)\n    else:\n        return f_beta\n\nboundary = ([0, 100], [100, 781], [786, 2706], [2706, 3474])\ncatagories = ('country', 'culture', 'medium', 'tags')\nfb = dict()\nfor i, catagory in enumerate(catagories):\n    fb[catagory]=[]\n    for i_ in range(2, 20, 2):\n        thre = 0.01*i_\n        fb[catagory].append(f2score(gt[:, boundary[i][0]: boundary[i][1]], probs[:, boundary[i][0]: boundary[i][1]], thre=thre))\n\n    len_ticks = len(fb[catagory])\n    x_axis = np.linspace(0.02, 0.2, len_ticks)\n    plt.figure()\n\n    plt.plot(x_axis, fb[catagory])\n    plt.title(\"Optimizing threshold to determine labels in catagory: '%s'\"\n             %catagory)\n    plt.xlabel('Threshold')\n    plt.ylabel('F2 Score')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f2score(gt, probs, thre=(0.04, 0.06, 0.08, 0.06))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}